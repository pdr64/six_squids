<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <title>Final Robot Design</title>
    <style>
      body{
        padding: 0 80px;}
    </style>
</head>
  <body>
    <div id= "header">
      <center><h1> Final Robot Design</h1></center>
    </div>
    <div id = "navbar">
      <a href="https://pdr64.github.io/six_squids/"><button class="btn"><i class="fa fa-home"></i> Back to Home Page</button></a>
    </div>
    <br>
    <br>
    
    <h3>Materials and Cost</h3>
    <h4>Materials List</h4>
    <ul>
      <li>3 Line Sensors</li>
      <li>3 Wall Sensors</li>
      <li>2 Parallax Continous Motion Servos</li>
      <li>2 Wheels</li>
      <li>1 D battery</li>
      <li>1 Power Bank</li>
      <li>1 DE0 Nano FGPA</li>
      <li>1 OVO7670 Camera</li>
      <li>1 Arduino Uno</li>
      <li>1 Arduino Nano</li>
    </ul>
    <h4>Final Cost</h4>
       <center>
      <table style="width:60%">
        <tr>
          <th>Component</th>
          <th>Cost</th> 
          <th>Quantity</th>
        </tr>
        <tr>
          <td>Ardino Uno</td>
          <td>16$</td> 
          <td>1</td>
        </tr>
        <tr>
          <td>Arduino Nano</td>
          <td>5$</td> 
          <td>1</td>
        </tr>
        <tr>
          <td>Line Sensor</td>
          <td>3$</td> 
          <td>3</td>
        </tr>
        <tr>
          <td>IR distance sensor</td>
          <td>7$</td> 
          <td>3</td>
        </tr>
        <tr>
          <td>Parallax Servos</td>
          <td>13$</td> 
          <td>2</td>
        </tr>
        <tr>
          <td>Camera</td>
          <td>14$</td> 
          <td>1</td>
        </tr>
      </table>
         </center>

    <h3>Basic Sensing</h3>
    <h4>Line Sensors</h4>
    <p>
      We decided to implement three line sensors. There are four possible scenarios given this configuration: when simply following a 
      straight line, we can either be perfectly on the line, in which case we simply continue going straight, or we can be veering left 
      or right slightly off the line, which is when the left or right sensor starts seeing white. To correct for this, we turn very slightly
      back in the direction towards the line. 
      <br>
      The fourth option is that we reach an intersection, in which case Gary launches a sequence of observations about his surroundings, 
      including wall detection and reporting information via radio about location and treasure detection. 
    </p>  
    <h4>Wall Sensors</h4>
    <p>
    We also decided to have three wall sensors. Initially we believed we should have a back wall sensor to detect if we started with a wall 
    behind us, but given the general configuration of the maze we determined this to be unecessary. At each intersection, Gary checks the front, 
    left and right wall sensors, by toggling a mux with the three analog inputs of the walls, and stores this information for both travel and 
    maze mapping purposes. 
    TODO: show schematic of mux
    </p>
    <h4>Robot Sensor (IR)</h4>
    <h4>Audio Sensor</h4>
    
    <h3>Chassis</h3>
    
    <h4>Propulsion</h4>
    <p>
      Batteries and Wheels
    </p> 
    <h4>Design</h4>
    <p>
      Aesthethics and structure 
    </p>
    <br>
    
    <h3>Algorithmic Navigation</h3>
    <h3>Treasure Detection</h3>
    <p>Getting the camera to detect treasures was a long and tedious process but ultimately, we have something we are happy with. 
      Here's how we got it to work.
    </p>
    <br>
    <h4>Getting the Cameras Operational</h4>
    <p>
      The first thing we did was get the Arduinos programmed to write registers to the camera. To do this we read through the 
      camera datasheet and found the right registers to set the data structure (RGB444), resolution (176x144), camera noise, using an 
      external clock, pixel format and more. We had difficulty with getting the right results from our camera and the problem proofed to be 
      from the registers we had set using outdated data sheet. Eventually, we were able to get the correct registers and a snippet of our
      code is shown below.
      <code><pre>
      	Serial.println("Writing registers");
    	Serial.println (OV7670_write_register(0x12, 0x80)); //COM7: Reset registers, enable color bar, resolution and pixel format 
    	delay(100);
    	Serial.println(OV7670_write_register(0x12, 0x0E)); //COM7: Reset registers, enable color bar, resolution and pixel format 
    	Serial.println(OV7670_write_register(0x0C, 0x08)); //COM3: Enable scaling
    	Serial.println(OV7670_write_register(0x14, 0x0B)); //COM9: To make the image less noisy
    	Serial.println(OV7670_write_register(0x11, 0xC0)); //CLKRC: Use external clock directly 
    	Serial.println(OV7670_write_register(0x40, 0xD0)); //COM15: pixel format
    	Serial.println(OV7670_write_register(0x42, 0x08)); //COM17: DSP color bar enable (0x42, 0x08)
   	Serial.println(OV7670_write_register(0x1E, 0x30)); //MVFP: Vertically flip image enable
    	Serial.println(OV7670_write_register(0x8C, 0x02)); //enable RGB444
      </pre></code>
    </p>

    <h4>Getting Data from the Camera</h4>
    <p> To run the Arduino program, we needed to protect the camera. We had  to disable the internal pull-up resistors that are a part of
      the Arduinoâ€™s I2C interface. This is because they pull the signals that set up our camera to 5V, while our camera requires 3.3V.
      Sending 5V through will harm the camera. We did this by locating the twi.c file at C:\Program Files (x86)\Arduino\hardware\arduino\avr\libraries\Wire\src\utility.
      Then we commented out the following lines of code:
      <code><pre>
       //activate internal pullups for twi
       digitalWrite(SDA,1);
       digitalWrite(SCL,1);
      </pre></code>
      <br>
      We read the camera data and the picture below shows the output. Comparing the output to the written values into the registers, we confirmed that our camera was working 
      correctly and is ready to send data into the DE0-Nano FPGA.
      <center><img src="registers.png" class="img-rounded" alt="Camera register output" width="600" height="450">
      <p><b>Fig.1:</b> Camera Register Output</p></center>
    </p>
    <br>
    <h3>Programming the FPGA to Read Camera Data</h3>
    <h4>Writing to M9K blocks</h4>
    <p>
	We tested our M9K block code which communicated with our VGA driver to output the colors which will be received from the camera. We wrote a few hard coded colors to be displayed on the monitor.
	The following example shows how it is done
	    <code><pre>
	    if(VGA_PIXEL_X>44 &&VGA_PIXEL_X<=66 &&VGA_PIXEL_Y<(`SCREEN_HEIGHT-1))begin
		pixel_data_RGB332 = 8'b11111100;  //set color
		W_EN = 1'b1;   //write enable
	    end
	    </pre></code>
	Below is an image of the color pattern we sent to the M9K block for display.
	<center><img src="color_patt.jpeg" class="img-rounded" alt="color pattern" width="300" height="450">
        <p><b>Fig.2:</b> Color pattern output</p></center>
	
    </p>
    <br>
    <h4>Downsampling</h4>
    <p>Our camera outputs a data structure of RGB444 in 16 bits. We however need only 8 bits out of the 16 bits in order to save on 
      memory. The Downsampling code collects all the data from the camera and selects only the most important bits necessary to display the colors we need.
      The OV7670 Camera can only output 8 bits of a pixel at a time through D7 - D0 (output connections). Using the camera clock cycle (see fig.3), we sampled down the output into RGB332.
      <center><img src="camera_clock.png" class="img-rounded" alt="Camera clock cycle" width="600" height="450">
      <p><b>Fig.3:</b> Clock cycle from the OV7670 Datasheet.This shows how the output from the clock is received based on time cycle</p></center>
      <br>Below is a snippet of our downsampler code. 
      <code><pre>
      //This set of codes receive camera data in RGB444 and downsamples it into RGB 332 by taking the important bits
      if (CAM_HREF_NEG) begin   //href clock is high
      	if (newByte == 1'b0) begin
				temp[7:0] = data;
				W_EN      = 1'b0;
				X_ADDR    = X_ADDR;
				newByte   = 1'b1;
				pixel_data_RGB332[4:0] = {data[7:5], data[3:2]};//get values for blue and green
			end
			else begin
				pixel_data_RGB332[7:5] = {data[3:1]};// get value for red
				X_ADDR = X_ADDR + 10'b1;
				W_EN = 1'b1;
				newByte = 1'b0;
			end
	end
      </pre></code>
	    The code above even so simple did not work on the first try. To check the correctness of the downsampling, we had to do a color bar test, which at first did not go as planned.
	    A compilation of our outcomes has been included in the video below. From that, it can be seen that we are close to obtaining all the bars except for the two last bars which were expected to be maroon and black. 
	    <center><iframe width="560" height="315" src="https://www.youtube.com/embed/cyY-qdVEeRE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>
    </p>
    <br>
    <h4>Reading Pixels</h4>
    <p>
	    In order to know what color our camera is seeing without using the VGA driver to output to screen, we need to read the pixels that come from the camera. We did so by implementing another code in our downsamplier to make the colors vivid. 
	    below is a diagram showing the logic behind the implementation. We used threshold values to make sure that our colors are truly present.
	    <center><img src="colorVivid.PNG" class="img-rounded" alt="Color vivid" width="600" height="450">
      	    <p><b>Fig.4:</b> Making our colors look right</p></center>
	    We do not care about green as shown in the diagram because there will not be any green treasures during the competition.
	    <b>Here is the result:</b>
	    <center><iframe width="560" height="315" src="https://www.youtube.com/embed/mjlxl8KlYC8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>
   	<br>
   	We wanted to read back to the Arduino data from the FPGA about the image such that the robot could report targets to the GUI 
        during competition. Therefore, we (did stuff on FPGA to analyze images) and sent that data in parallel back to the Arduino
        <br>
    	When the Arduino recieves this data, it analyzed the information about whether or not there was a treasure, what color 
        it is, and the shape it is (out of 7 possible combinations) and eventually in milestone 4 we intend to include this in our search 
        path and incorporate it into the information we send to the GUI. 
    </p>
    <br>
    
    
    
    
   <a href="#header"><button class="btn"><i class="fa fa-arrow-up"></i>Return to top</button></a>
       
 </body>
</html>
